{
  "hash": "70abc170209bb33efe3009ceb8984fe3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Decision Trees Vs. Random Forests\"\nauthor: \"Gurmehak\"\ndate: \"2025-01-18\"\ncategories: [code, analysis]\nimage: \"thumbnail.jpg\"\n---\n\n\n# Decision Trees or Random Forests: Which One to Choose?\n\n## Introduction \n\nEverybody is working on building machine learning models, but what distinguishes an experienced practitioner from a newcomer is the depth of their understanding of algorithms (max_depth of knowledge = 'inf'). Among the many available algorithms, Decision Trees and Random Forests are two of the most commonly used. These models are valued for their simplicity and powerful predictive capabilities, making them popular across industries such as healthcare, finance, marketing, and more.\n\nBoth Decision Trees and Random Forests belong to the broader family of supervised learning algorithms. While they share foundational principles, their outcomes can differ significantly, which can impact the performance of your model. Understanding when and why to use each model can make a substantial difference in your machine learning pipeline.\n\nBut what‚Äôs the logic behind these algorithms, and how do you decide when to use one over the other? No, Decision Trees aren't superior simply because they \"decide\" things, and Random Forests aren‚Äôt inferior because their outputs are \"random.\" There is a deep, well-thought-out logic that drives these algorithms (Random Forests are just a bunch of democratic decision trees). In this article, we will break down both models, explaining their workings, strengths, weaknesses, and ideal use cases. Hopefully by the end of this blog you will undertsand my max_depth joke and be able to make an informed decision when faced with the choice: Decision Tree or Random Forest?\n\n<img src=\"thumbnail.jpg\" width=\"600\" />\n\n*Image credit: Abhishek Sharma from [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/)\n\n## What are Decision Trees?\n\nA Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by splitting the data one feature at a time. At each step, the tree makes a decision based on a threshold for that feature, moving from the root node through a series of decisions until it reaches a final prediction. This process creates a tree-like structure from the root to the leaves.\nThink of it like this: when guessing a pizza topping...<br>\nIs it green? ‚ûî Nope!<br>\nIs it yellow? ‚ûî Yup!<br>\nIs it sweet? ‚ûî Yeah!<br>\nIs it tropical? ‚ûî You bet!<br>\nDoes it go on pizza? ‚ûî Absolutely!<br>\nIt's pineapple! üçç<br>\n\nThe decision on each node is made by minimizing the Gini index (or impurity) to maximize information gain, which measures how well a given feature separates the data according to the target classification.\n\n#### Strengths\n- **Interpretability**: Decision Trees are highly interpretable. You can visualize the decision-making process, making it easy to understand how predictions are made.\n- **Works Well with Non-linear Data**: Decision Trees can model complex decision boundaries, making them effective for tasks where relationships between features are non-linear.\n- **Easy to Work With**: Decision Trees handle missing values seamlessly and do not require scaling or normalization. They are also invariant to feature scaling and do not need to address multicollinearity.\n\n#### Weaknesses\n- **Prone to Overfitting**: Decision Trees tend to overfit, especially when the tree is deep. They can become too complex and memorize the training data, leading to poor generalization on unseen data.\n- **Instability**: Small changes in the data can lead to large changes in the structure of the tree, making them less robust.\n- **Greedy Nature**: Decision trees are greedy algorithms, meaning they make the locally optimal choice at each node, which doesn‚Äôt always lead to the globally optimal tree.\n\n#### Code Snippet\n\n::: {#1fd0d0b2 .cell execution_count=1}\n``` {.python .cell-code}\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Decision Tree\nstart_time = time.time()\ntree = DecisionTreeClassifier(random_state=42, max_depth=2)\ntree.fit(X_train, y_train)\ntree_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = tree.predict(X_test)\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Decision Tree: {tree_time:.4f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Accuracy: 0.9833\nTime taken for Decision Tree: 0.0018 seconds\n```\n:::\n:::\n\n\nWhere `max_depth`: The maximum depth of the tree. None means the tree will continue expanding until all leaves are pure or contain fewer than min_samples_split samples.\n\n\n## What are Random Forests?\n\nA Random Forest is an ensemble learning method that builds multiple Decision Trees and combines their results to make a final prediction. Each tree in the forest is a decision tree like the ones we discussed earlier, but instead of making the final decision on its own, each tree \"votes.\" The idea is that by combining many weak models (individual Decision Trees), the ensemble model can make a more accurate and robust prediction.\nRandom Forests use two main techniques to improve accuracy and reduce variance:\n\n* Bootstrapping: Each tree is trained on a random subset of the original data, chosen with replacement. This process is called bagging (bootstrap aggregating).\n* Feature Randomness: At each split, a random subset of features is considered, which helps reduce correlation between trees and improves generalization.\n\nRecall how one decision tree identified pineapple on pizza. This is how a random fores makes it decisions...\n\nTree 1: Pineapple üçç<br>\nTree 2: Mango ü•≠<br>\nTree 3: Pineapple üçç<br>\nTree 4: Pineapple üçç<br>\nTree 5: Mango ü•≠<br>\n\nThe Random Forest will make its final prediction based on the majority vote (in this case, Pineapple üçç).\n\n#### Strengths\n- **Accuracy**: Random Forests typically outperform individual Decision Trees by reducing overfitting and providing better generalization to new data.\n- **Robustness**: The use of multiple trees makes Random Forests less sensitive to noisy data. Even if some trees perform poorly, the overall model remains strong.\n- **Automatic Feature Selection**: Random Forests can identify the most important features for making predictions, helping to focus on the most relevant variables.\n\n#### Weaknesses\n- **Interpretability**: Random Forests are harder to interpret than single Decision Trees. While feature importance can be determined, understanding how each feature contributes across many trees is more complex.\n- **Computation Time**: Training a Random Forest involves building many Decision Trees, which can be computationally expensive, especially with large datasets.\n- **Memory Usage***: The model size can grow quickly as more trees are added, requiring significant memory and storage.\n\n#### Code Snippet\n\n::: {#108fe56b .cell execution_count=2}\n``` {.python .cell-code}\nimport time\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Random Forest\nstart_time = time.time()\nforest = RandomForestClassifier(n_estimators=20, max_depth= 2, random_state=42)\nforest.fit(X_train, y_train)\nforest_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = forest.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Random Forest: {forest_time:.4f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Accuracy: 0.9833\nTime taken for Random Forest: 0.0130 seconds\n```\n:::\n:::\n\n\nWhere `n_estimators`: The number of decision trees in the forest, <br>\n`criterion`: The function to measure the quality of a split (like 'gini' for Gini impurity) and <br>\n`max_depth`: The maximum depth of the trees. None means the tree will expand until all leaves are pure or contain fewer than min_samples_split samples.\n\n\n## Decision Tree vs. Random Forest: The Ultimate Guide\n\n| **Use Case**                                   | **Verdict**      | **Reason**                                                                                             |\n|------------------------------------------------|------------------|--------------------------------------------------------------------------------------------------------|\n| When you want to train the model faster        | Decision Tree    | Decision Trees train faster because they use a single tree structure, while Random Forest builds multiple trees. |\n| When you want to reduce overfitting            | Random Forest    | Random Forest reduces overfitting by averaging the results of multiple trees, reducing variance.         |\n| When you want interpretability                 | Decision Tree    | Decision Trees are more interpretable and easier to visualize, making them ideal for explainable models. |\n| When your data has complex relationships       | Random Forest    | Random Forest can capture complex nonlinear relationships and interactions between features.            |\n| When you need a robust model                   | Random Forest    | Random Forest averages multiple decision trees, making it more robust to noise and overfitting.          |\n| When you want to handle missing values         | Random Forest    | Random Forest can impute missing values by leveraging different trees, improving its robustness.         |\n| When you need real-time predictions            | Decision Tree    | Decision Trees are faster for real-time predictions because they only require traversing a single tree.  |\n| When model complexity is a concern             | Decision Tree    | A single Decision Tree is simpler and requires less computational power than Random Forest.             |\n| When working with large datasets               | Random Forest    | Random Forest is better for large datasets because it handles high-dimensional data and reduces variance. |\n| When you want feature importance               | Random Forest    | Random Forest provides more reliable feature importance metrics by aggregating across multiple trees.    |\n| When you need computational efficiency         | Decision Tree    | Decision Trees are more computationally efficient and require less memory compared to Random Forests.   |\n| When you need out-of-bag error estimation      | Random Forest    | Random Forest has built-in out-of-bag (OOB) error estimation, allowing for model validation without a separate validation set. |\n\n\n## Real-word Applications\n\n* **Healthcare**: <br>\n**Decision Trees**: Used for creating simple diagnostic rules, like checking if a patient has symptoms A, B, and C to predict a condition (e.g., flu or no flu). <br>\n**Random Forests**: Applied to more complex tasks, like predicting the risk of diseases based on a variety of factors such as lifestyle, genetics, and environmental data.\n<br>\n* **Finance**:<br>\n**Decision Trees**: Help decide whether a loan should be approved based on factors like income and credit score.\n**Random Forests**: Used to identify fraud in transactions by considering multiple factors, such as past behavior, transaction amount, and location, making the prediction more accurate.\n\n## Conclusion\nTo sum up, the choice between Decision Trees and Random Forests depends on your model's needs. For more complex data, higher accuracy, and reduced overfitting, the more, the merrier‚ÄîRandom Forests are the way to go. But for simpler, faster, and more interpretable predictions, too many cooks spoil the broth, and Decision Trees are ideal.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}