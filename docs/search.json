[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "Everybody is working on building machine learning models, but what distinguishes an experienced practitioner from a newcomer is the depth of their understanding of algorithms (max_depth of knowledge = ‚Äòinf‚Äô). Among the many available algorithms, Decision Trees and Random Forests are two of the most commonly used. These models are valued for their simplicity and powerful predictive capabilities, making them popular across industries such as healthcare, finance, marketing, and more.\nBoth Decision Trees and Random Forests belong to the broader family of supervised learning algorithms. While they share foundational principles, their outcomes can differ significantly, which can impact the performance of your model. Understanding when and why to use each model can make a substantial difference in your machine learning pipeline.\nBut what‚Äôs the logic behind these algorithms, and how do you decide when to use one over the other? No, Decision Trees aren‚Äôt superior simply because they ‚Äúdecide‚Äù things, and Random Forests aren‚Äôt inferior because their outputs are ‚Äúrandom.‚Äù There is a deep, well-thought-out logic that drives these algorithms (Random Forests are just a bunch of democratic decision trees). In this article, we will break down both models, explaining their workings, strengths, weaknesses, and ideal use cases. Hopefully by the end of this blog you will undertsand my max_depth joke and be able to make an informed decision when faced with the choice: Decision Tree or Random Forest?\n\n*Image credit: Abhishek Sharma from AnalyticsVidhya\n\n\n\nA Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by splitting the data one feature at a time. At each step, the tree makes a decision based on a threshold for that feature, moving from the root node through a series of decisions until it reaches a final prediction. This process creates a tree-like structure from the root to the leaves. Think of it like this: when guessing a pizza topping‚Ä¶ Is it green? ‚ûî Nope! Is it yellow? ‚ûî Yup! Is it sweet? ‚ûî Yeah! Is it tropical? ‚ûî You bet! Does it go on pizza? ‚ûî Absolutely! It‚Äôs pineapple! üçç\nThe decision on each node is made by minimizing the Gini index (or impurity) to maximize information gain, which measures how well a given feature separates the data according to the target classification.\n\n\n\nInterpretability: Decision Trees are highly interpretable. You can visualize the decision-making process, making it easy to understand how predictions are made.\nWorks Well with Non-linear Data: Decision Trees can model complex decision boundaries, making them effective for tasks where relationships between features are non-linear.\nEasy to Work With: Decision Trees handle missing values seamlessly and do not require scaling or normalization. They are also invariant to feature scaling and do not need to address multicollinearity.\n\n\n\n\n\nProne to Overfitting: Decision Trees tend to overfit, especially when the tree is deep. They can become too complex and memorize the training data, leading to poor generalization on unseen data.\nInstability: Small changes in the data can lead to large changes in the structure of the tree, making them less robust.\nGreedy Nature: Decision trees are greedy algorithms, meaning they make the locally optimal choice at each node, which doesn‚Äôt always lead to the globally optimal tree.\n\n\n\n\n\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Decision Tree\nstart_time = time.time()\ntree = DecisionTreeClassifier(random_state=42, max_depth=2)\ntree.fit(X_train, y_train)\ntree_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = tree.predict(X_test)\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Decision Tree: {tree_time:.4f} seconds\")\n\nDecision Tree Accuracy: 0.9833\nTime taken for Decision Tree: 0.0018 seconds\n\n\nWhere max_depth: The maximum depth of the tree. None means the tree will continue expanding until all leaves are pure or contain fewer than min_samples_split samples.\n\n\n\n\nA Random Forest is an ensemble learning method that builds multiple Decision Trees and combines their results to make a final prediction. Each tree in the forest is a decision tree like the ones we discussed earlier, but instead of making the final decision on its own, each tree ‚Äúvotes.‚Äù The idea is that by combining many weak models (individual Decision Trees), the ensemble model can make a more accurate and robust prediction. Random Forests use two main techniques to improve accuracy and reduce variance:\n\nBootstrapping: Each tree is trained on a random subset of the original data, chosen with replacement. This process is called bagging (bootstrap aggregating).\nFeature Randomness: At each split, a random subset of features is considered, which helps reduce correlation between trees and improves generalization.\n\nRecall how one decision tree identified pineapple on pizza. This is how a random fores makes it decisions‚Ä¶\nTree 1: Pineapple üçç Tree 2: Mango ü•≠ Tree 3: Pineapple üçç Tree 4: Pineapple üçç Tree 5: Mango ü•≠\nThe Random Forest will make its final prediction based on the majority vote (in this case, Pineapple üçç).\n\n\n\nAccuracy: Random Forests typically outperform individual Decision Trees by reducing overfitting and providing better generalization to new data.\nRobustness: The use of multiple trees makes Random Forests less sensitive to noisy data. Even if some trees perform poorly, the overall model remains strong.\nAutomatic Feature Selection: Random Forests can identify the most important features for making predictions, helping to focus on the most relevant variables.\n\n\n\n\n\nInterpretability: Random Forests are harder to interpret than single Decision Trees. While feature importance can be determined, understanding how each feature contributes across many trees is more complex.\nComputation Time: Training a Random Forest involves building many Decision Trees, which can be computationally expensive, especially with large datasets.\nMemory Usage*: The model size can grow quickly as more trees are added, requiring significant memory and storage.\n\n\n\n\n\nimport time\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Random Forest\nstart_time = time.time()\nforest = RandomForestClassifier(n_estimators=20, max_depth= 2, random_state=42)\nforest.fit(X_train, y_train)\nforest_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = forest.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Random Forest: {forest_time:.4f} seconds\")\n\nRandom Forest Accuracy: 0.9833\nTime taken for Random Forest: 0.0130 seconds\n\n\nWhere n_estimators: The number of decision trees in the forest,  criterion: The function to measure the quality of a split (like ‚Äògini‚Äô for Gini impurity) and  max_depth: The maximum depth of the trees. None means the tree will expand until all leaves are pure or contain fewer than min_samples_split samples.\n\n\n\n\n\n\n\n\n\n\n\n\nUse Case\nVerdict\nReason\n\n\n\n\nWhen you want to train the model faster\nDecision Tree\nDecision Trees train faster because they use a single tree structure, while Random Forest builds multiple trees.\n\n\nWhen you want to reduce overfitting\nRandom Forest\nRandom Forest reduces overfitting by averaging the results of multiple trees, reducing variance.\n\n\nWhen you want interpretability\nDecision Tree\nDecision Trees are more interpretable and easier to visualize, making them ideal for explainable models.\n\n\nWhen your data has complex relationships\nRandom Forest\nRandom Forest can capture complex nonlinear relationships and interactions between features.\n\n\nWhen you need a robust model\nRandom Forest\nRandom Forest averages multiple decision trees, making it more robust to noise and overfitting.\n\n\nWhen you want to handle missing values\nRandom Forest\nRandom Forest can impute missing values by leveraging different trees, improving its robustness.\n\n\nWhen you need real-time predictions\nDecision Tree\nDecision Trees are faster for real-time predictions because they only require traversing a single tree.\n\n\nWhen model complexity is a concern\nDecision Tree\nA single Decision Tree is simpler and requires less computational power than Random Forest.\n\n\nWhen working with large datasets\nRandom Forest\nRandom Forest is better for large datasets because it handles high-dimensional data and reduces variance.\n\n\nWhen you want feature importance\nRandom Forest\nRandom Forest provides more reliable feature importance metrics by aggregating across multiple trees.\n\n\nWhen you need computational efficiency\nDecision Tree\nDecision Trees are more computationally efficient and require less memory compared to Random Forests.\n\n\nWhen you need out-of-bag error estimation\nRandom Forest\nRandom Forest has built-in out-of-bag (OOB) error estimation, allowing for model validation without a separate validation set.\n\n\n\n\n\n\n\nHealthcare:  Decision Trees: Used for creating simple diagnostic rules, like checking if a patient has symptoms A, B, and C to predict a condition (e.g., flu or no flu).  Random Forests: Applied to more complex tasks, like predicting the risk of diseases based on a variety of factors such as lifestyle, genetics, and environmental data. \nFinance: Decision Trees: Help decide whether a loan should be approved based on factors like income and credit score. Random Forests: Used to identify fraud in transactions by considering multiple factors, such as past behavior, transaction amount, and location, making the prediction more accurate.\n\n\n\n\nTo sum up, the choice between Decision Trees and Random Forests depends on your model‚Äôs needs. For more complex data, higher accuracy, and reduced overfitting, the more, the merrier‚ÄîRandom Forests are the way to go. But for simpler, faster, and more interpretable predictions, too many cooks spoil the broth, and Decision Trees are ideal."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "Everybody is working on building machine learning models, but what distinguishes an experienced practitioner from a newcomer is the depth of their understanding of algorithms (max_depth of knowledge = ‚Äòinf‚Äô). Among the many available algorithms, Decision Trees and Random Forests are two of the most commonly used. These models are valued for their simplicity and powerful predictive capabilities, making them popular across industries such as healthcare, finance, marketing, and more.\nBoth Decision Trees and Random Forests belong to the broader family of supervised learning algorithms. While they share foundational principles, their outcomes can differ significantly, which can impact the performance of your model. Understanding when and why to use each model can make a substantial difference in your machine learning pipeline.\nBut what‚Äôs the logic behind these algorithms, and how do you decide when to use one over the other? No, Decision Trees aren‚Äôt superior simply because they ‚Äúdecide‚Äù things, and Random Forests aren‚Äôt inferior because their outputs are ‚Äúrandom.‚Äù There is a deep, well-thought-out logic that drives these algorithms (Random Forests are just a bunch of democratic decision trees). In this article, we will break down both models, explaining their workings, strengths, weaknesses, and ideal use cases. Hopefully by the end of this blog you will undertsand my max_depth joke and be able to make an informed decision when faced with the choice: Decision Tree or Random Forest?\n\n*Image credit: Abhishek Sharma from AnalyticsVidhya"
  },
  {
    "objectID": "posts/post-with-code/index.html#what-are-decision-trees",
    "href": "posts/post-with-code/index.html#what-are-decision-trees",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by splitting the data one feature at a time. At each step, the tree makes a decision based on a threshold for that feature, moving from the root node through a series of decisions until it reaches a final prediction. This process creates a tree-like structure from the root to the leaves. Think of it like this: when guessing a pizza topping‚Ä¶ Is it green? ‚ûî Nope! Is it yellow? ‚ûî Yup! Is it sweet? ‚ûî Yeah! Is it tropical? ‚ûî You bet! Does it go on pizza? ‚ûî Absolutely! It‚Äôs pineapple! üçç\nThe decision on each node is made by minimizing the Gini index (or impurity) to maximize information gain, which measures how well a given feature separates the data according to the target classification.\n\n\n\nInterpretability: Decision Trees are highly interpretable. You can visualize the decision-making process, making it easy to understand how predictions are made.\nWorks Well with Non-linear Data: Decision Trees can model complex decision boundaries, making them effective for tasks where relationships between features are non-linear.\nEasy to Work With: Decision Trees handle missing values seamlessly and do not require scaling or normalization. They are also invariant to feature scaling and do not need to address multicollinearity.\n\n\n\n\n\nProne to Overfitting: Decision Trees tend to overfit, especially when the tree is deep. They can become too complex and memorize the training data, leading to poor generalization on unseen data.\nInstability: Small changes in the data can lead to large changes in the structure of the tree, making them less robust.\nGreedy Nature: Decision trees are greedy algorithms, meaning they make the locally optimal choice at each node, which doesn‚Äôt always lead to the globally optimal tree.\n\n\n\n\n\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Decision Tree\nstart_time = time.time()\ntree = DecisionTreeClassifier(random_state=42, max_depth=2)\ntree.fit(X_train, y_train)\ntree_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = tree.predict(X_test)\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Decision Tree: {tree_time:.4f} seconds\")\n\nDecision Tree Accuracy: 0.9833\nTime taken for Decision Tree: 0.0018 seconds\n\n\nWhere max_depth: The maximum depth of the tree. None means the tree will continue expanding until all leaves are pure or contain fewer than min_samples_split samples."
  },
  {
    "objectID": "posts/post-with-code/index.html#what-are-random-forests",
    "href": "posts/post-with-code/index.html#what-are-random-forests",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "A Random Forest is an ensemble learning method that builds multiple Decision Trees and combines their results to make a final prediction. Each tree in the forest is a decision tree like the ones we discussed earlier, but instead of making the final decision on its own, each tree ‚Äúvotes.‚Äù The idea is that by combining many weak models (individual Decision Trees), the ensemble model can make a more accurate and robust prediction. Random Forests use two main techniques to improve accuracy and reduce variance:\n\nBootstrapping: Each tree is trained on a random subset of the original data, chosen with replacement. This process is called bagging (bootstrap aggregating).\nFeature Randomness: At each split, a random subset of features is considered, which helps reduce correlation between trees and improves generalization.\n\nRecall how one decision tree identified pineapple on pizza. This is how a random fores makes it decisions‚Ä¶\nTree 1: Pineapple üçç Tree 2: Mango ü•≠ Tree 3: Pineapple üçç Tree 4: Pineapple üçç Tree 5: Mango ü•≠\nThe Random Forest will make its final prediction based on the majority vote (in this case, Pineapple üçç).\n\n\n\nAccuracy: Random Forests typically outperform individual Decision Trees by reducing overfitting and providing better generalization to new data.\nRobustness: The use of multiple trees makes Random Forests less sensitive to noisy data. Even if some trees perform poorly, the overall model remains strong.\nAutomatic Feature Selection: Random Forests can identify the most important features for making predictions, helping to focus on the most relevant variables.\n\n\n\n\n\nInterpretability: Random Forests are harder to interpret than single Decision Trees. While feature importance can be determined, understanding how each feature contributes across many trees is more complex.\nComputation Time: Training a Random Forest involves building many Decision Trees, which can be computationally expensive, especially with large datasets.\nMemory Usage*: The model size can grow quickly as more trees are added, requiring significant memory and storage.\n\n\n\n\n\nimport time\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Train Random Forest\nstart_time = time.time()\nforest = RandomForestClassifier(n_estimators=20, max_depth= 2, random_state=42)\nforest.fit(X_train, y_train)\nforest_time = time.time() - start_time\n\n# Predict and evaluate\ny_pred = forest.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"Time taken for Random Forest: {forest_time:.4f} seconds\")\n\nRandom Forest Accuracy: 0.9833\nTime taken for Random Forest: 0.0130 seconds\n\n\nWhere n_estimators: The number of decision trees in the forest,  criterion: The function to measure the quality of a split (like ‚Äògini‚Äô for Gini impurity) and  max_depth: The maximum depth of the trees. None means the tree will expand until all leaves are pure or contain fewer than min_samples_split samples."
  },
  {
    "objectID": "posts/post-with-code/index.html#decision-tree-vs.-random-forest-the-ultimate-guide",
    "href": "posts/post-with-code/index.html#decision-tree-vs.-random-forest-the-ultimate-guide",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "Use Case\nVerdict\nReason\n\n\n\n\nWhen you want to train the model faster\nDecision Tree\nDecision Trees train faster because they use a single tree structure, while Random Forest builds multiple trees.\n\n\nWhen you want to reduce overfitting\nRandom Forest\nRandom Forest reduces overfitting by averaging the results of multiple trees, reducing variance.\n\n\nWhen you want interpretability\nDecision Tree\nDecision Trees are more interpretable and easier to visualize, making them ideal for explainable models.\n\n\nWhen your data has complex relationships\nRandom Forest\nRandom Forest can capture complex nonlinear relationships and interactions between features.\n\n\nWhen you need a robust model\nRandom Forest\nRandom Forest averages multiple decision trees, making it more robust to noise and overfitting.\n\n\nWhen you want to handle missing values\nRandom Forest\nRandom Forest can impute missing values by leveraging different trees, improving its robustness.\n\n\nWhen you need real-time predictions\nDecision Tree\nDecision Trees are faster for real-time predictions because they only require traversing a single tree.\n\n\nWhen model complexity is a concern\nDecision Tree\nA single Decision Tree is simpler and requires less computational power than Random Forest.\n\n\nWhen working with large datasets\nRandom Forest\nRandom Forest is better for large datasets because it handles high-dimensional data and reduces variance.\n\n\nWhen you want feature importance\nRandom Forest\nRandom Forest provides more reliable feature importance metrics by aggregating across multiple trees.\n\n\nWhen you need computational efficiency\nDecision Tree\nDecision Trees are more computationally efficient and require less memory compared to Random Forests.\n\n\nWhen you need out-of-bag error estimation\nRandom Forest\nRandom Forest has built-in out-of-bag (OOB) error estimation, allowing for model validation without a separate validation set."
  },
  {
    "objectID": "posts/post-with-code/index.html#real-word-applications",
    "href": "posts/post-with-code/index.html#real-word-applications",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "Healthcare:  Decision Trees: Used for creating simple diagnostic rules, like checking if a patient has symptoms A, B, and C to predict a condition (e.g., flu or no flu).  Random Forests: Applied to more complex tasks, like predicting the risk of diseases based on a variety of factors such as lifestyle, genetics, and environmental data. \nFinance: Decision Trees: Help decide whether a loan should be approved based on factors like income and credit score. Random Forests: Used to identify fraud in transactions by considering multiple factors, such as past behavior, transaction amount, and location, making the prediction more accurate."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Decision Trees Vs. Random Forests",
    "section": "",
    "text": "To sum up, the choice between Decision Trees and Random Forests depends on your model‚Äôs needs. For more complex data, higher accuracy, and reduced overfitting, the more, the merrier‚ÄîRandom Forests are the way to go. But for simpler, faster, and more interpretable predictions, too many cooks spoil the broth, and Decision Trees are ideal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "decision-tree-vs-random-forest",
    "section": "",
    "text": "Decision Trees Vs. Random Forests\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nGurmehak\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  }
]